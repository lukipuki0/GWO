{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3M65w0lwMzYhG49Hy/WAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukipuki0/GWO/blob/main/GWO_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIXMql-BhbSQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.special import gamma\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Librerias para CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Configuración de visualización de Pandas  (No para HPC)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#pd.set_option('display.width', 200)\n",
        "#pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#************************************************************************************************************\n",
        "# *************************FUNCION PARA CNN *****************************************************************\n",
        "def train_cnn_model(num_conv_layers, base_filter_value, use_batch_norm, lr, batch_size, epochs):\n",
        "    start_time = time.time()  # inicio tiempo\n",
        "    now = datetime.datetime.now()\n",
        "    formatted_date = now.strftime('%Y%m%d_%H%M%S')\n",
        "    #Directorio donde se guardarán las ejecuiones de CNN --cambiar nombre segun configuracion de esquema a aprobar\n",
        "\n",
        "    base_directory = 'Esquema1'  # Ruta directa en HPC\n",
        "    #base_directory='/content/drive/MyDrive/Colab Notebooks/Esquema1'\n",
        "\n",
        "    folder_name = os.path.join(base_directory, f'ejecucion_cnn_{formatted_date}')\n",
        "\n",
        "    # Revisar existencia de directorio\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "    log_file_path = os.path.join(folder_name, 'training_log.txt')\n",
        "\n",
        "    class CustomCNN(nn.Module):\n",
        "        def __init__(self, num_conv_layers, base_filter_value, use_batch_norm):\n",
        "            super(CustomCNN, self).__init__()\n",
        "            layers = []\n",
        "            in_channels = 3  # Assuming input images have 3 channels (RGB)\n",
        "            current_filters = base_filter_value\n",
        "\n",
        "            for i in range(num_conv_layers):\n",
        "                layers.append(nn.Conv2d(in_channels, current_filters, kernel_size=3, padding=1))\n",
        "                if use_batch_norm:\n",
        "                    layers.append(nn.BatchNorm2d(current_filters))\n",
        "                layers.append(nn.ReLU(inplace=True))\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "                in_channels = current_filters\n",
        "                current_filters *= 2  # Double the number of filters for the next layer\n",
        "\n",
        "            self.features = nn.Sequential(*layers)\n",
        "            self.fc1 = nn.Linear(in_channels * (64 // 2**num_conv_layers)**2, 1024)\n",
        "            self.dropout = nn.Dropout(0.5)\n",
        "            self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.dropout(x)\n",
        "            x = self.fc2(x)\n",
        "            return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Data loading and transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    #Ruta de acceso local en colab\n",
        "    #train_path = '/content/drive/MyDrive/Colab Notebooks/dataset/train'\n",
        "    #valid_path ='/content/drive/MyDrive/Colab Notebooks/dataset/valid'\n",
        "    #test_path = '/content/drive/MyDrive/Colab Notebooks/dataset/test'\n",
        "\n",
        "    #Ruta de acceso en HPC\n",
        "    train_path = '../../Datasets/dataset_experimental/train'\n",
        "    valid_path = '../../Datasets/dataset_experimental/valid'\n",
        "    test_path = '../../Datasets/dataset_experimental/test'\n",
        "\n",
        "    train_data = datasets.ImageFolder(train_path, transform=transform)\n",
        "    valid_data = datasets.ImageFolder(valid_path, transform=transform)\n",
        "    test_data = datasets.ImageFolder(test_path, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_data, batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
        "\n",
        "    model = CustomCNN(num_conv_layers, base_filter_value, use_batch_norm)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loss_history, valid_loss_history, valid_accuracy_history = [], [], []\n",
        "\n",
        "    with open(log_file_path, 'w') as log_file:\n",
        "        print('\\nResultados por epochs')\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "            for data, target in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            average_train_loss = total_train_loss / len(train_loader.dataset)\n",
        "            train_loss_history.append(average_train_loss)\n",
        "\n",
        "            model.eval()\n",
        "            total_valid_loss, valid_correct, total_valid_samples = 0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in valid_loader:\n",
        "                    output = model(data)\n",
        "                    loss = criterion(output, target)\n",
        "                    total_valid_loss += loss.item() * data.size(0)\n",
        "                    pred = output.argmax(dim=1, keepdim=True)\n",
        "                    valid_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "                    total_valid_samples += data.size(0)\n",
        "\n",
        "            average_valid_loss = total_valid_loss / total_valid_samples\n",
        "            valid_accuracy = 100. * valid_correct / total_valid_samples\n",
        "            valid_loss_history.append(average_valid_loss)\n",
        "            valid_accuracy_history.append(valid_accuracy)\n",
        "            log_msg = f'Epoch {epoch+1}/{epochs}, Training Loss: {average_train_loss:.4f}, Validation Loss: {average_valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.2f}%\\n'\n",
        "            log_file.write(log_msg)\n",
        "            print(log_msg)\n",
        "\n",
        "        # Calculate the final average values\n",
        "        final_average_valid_loss = sum(valid_loss_history) / len(valid_loss_history)\n",
        "        final_average_valid_accuracy = sum(valid_accuracy_history) / len(valid_accuracy_history)\n",
        "\n",
        "        # Calculate total execution time\n",
        "        end_time = time.time()\n",
        "        total_execution_time = end_time - start_time\n",
        "\n",
        "        # Convert execution time to hours, minutes, and seconds\n",
        "        hours, rem = divmod(total_execution_time, 3600)\n",
        "        minutes, seconds = divmod(rem, 60)\n",
        "        time_formatted = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n",
        "\n",
        "        # Log final results\n",
        "        final_log_msg = f'\\nFinal Average Validation Loss: {final_average_valid_loss:.4f}\\n'\n",
        "        final_log_msg += f'Final Average Validation Accuracy: {final_average_valid_accuracy:.2f}%\\n'\n",
        "        final_log_msg += f'Total Execution Time: {time_formatted} (hh:mm:ss)\\n'\n",
        "        log_file.write(final_log_msg)\n",
        "        print(final_log_msg)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_loss_history, label='Training Loss')\n",
        "    plt.plot(valid_loss_history, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(folder_name, 'loss_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot validation accuracy\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(valid_accuracy_history, label='Validation Accuracy')\n",
        "    plt.title('Validation Accuracy per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(folder_name, 'accuracy_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return final_average_valid_loss, final_average_valid_accuracy, total_execution_time\n",
        "\n",
        "\n",
        "\n",
        "#**********************************************************************************************************************\n",
        "#********************************************   GREY    WOLF    OPTIMAZER   *******************************************\n",
        "\n",
        "# Inicializar población de lobos (soluciones aleatorias)\n",
        "def initialize_population(nwolves, param_ranges):\n",
        "    population = []\n",
        "    for _ in range(nwolves):\n",
        "        wolf = {}\n",
        "        for param in param_ranges:\n",
        "            if isinstance(param_ranges[param], list):\n",
        "                wolf[param] = random.choice(param_ranges[param])\n",
        "            else:\n",
        "                wolf[param] = random.uniform(*param_ranges[param])\n",
        "        population.append(wolf)\n",
        "    return population\n",
        "\n",
        "# Grey Wolf Optimizer (GWO)\n",
        "def gwo_optimize(nwolves, max_iter, w1, w2, w3, param_ranges):\n",
        "    a = 2  # Coeficiente inicial para GWO\n",
        "\n",
        "    # Inicialización de la población de lobos\n",
        "    wolves = initialize_population(nwolves, param_ranges)\n",
        "\n",
        "    for t in range(max_iter):\n",
        "        a = 2 - t * ((2) / maxIter)  # Disminución de a linealmente de 2 a 0\n",
        "        print(\"\\n**********************************************  ITERACION \", i+1, \" **********************************************\")\n",
        "\n",
        "        # Evaluar el fitness de cada lobo (hiperparámetros)\n",
        "        fitness_scores = []\n",
        "        for wolf in wolves:\n",
        "            accuracy, loss, time_taken = train_cnn_model(\n",
        "                num_conv_layers=wolf['num_layers'],\n",
        "                base_filter_value=wolf['num_filters'],\n",
        "                use_batch_norm=wolf['batch_norm'],\n",
        "                lr=wolf['lr'],\n",
        "                batch_size=wolf['batch_size'],\n",
        "                epochs=wolf['epochs']\n",
        "            )\n",
        "            fitness = w1 * (1 - accuracy / 100) + w2 * loss + w3 * time_taken\n",
        "            fitness_scores.append(fitness)\n",
        "\n",
        "        # Ordenar lobos según fitness (menor es mejor)\n",
        "        sorted_wolves = sorted(zip(fitness_scores, wolves), key=lambda x: x[0])\n",
        "        alpha, beta, delta = sorted_wolves[0][1], sorted_wolves[1][1], sorted_wolves[2][1]\n",
        "\n",
        "        # Actualizar posiciones de los lobos restantes\n",
        "        for i, wolf in enumerate(wolves):\n",
        "            for param in wolf:\n",
        "                if isinstance(wolf[param], (int, float)):\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A1 = 2 * a * r1 - a\n",
        "                    C1 = 2 * r2\n",
        "                    D_alpha = abs(C1 * alpha[param] - wolf[param])\n",
        "                    X1 = alpha[param] - A1 * D_alpha\n",
        "\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A2 = 2 * a * r1 - a\n",
        "                    C2 = 2 * r2\n",
        "                    D_beta = abs(C2 * beta[param] - wolf[param])\n",
        "                    X2 = beta[param] - A2 * D_beta\n",
        "\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A3 = 2 * a * r1 - a\n",
        "                    C3 = 2 * r2\n",
        "                    D_delta = abs(C3 * delta[param] - wolf[param])\n",
        "                    X3 = delta[param] - A3 * D_delta\n",
        "\n",
        "                    # Actualización de la posición\n",
        "                    wolf[param] = (X1 + X2 + X3) / 3\n",
        "\n",
        "\n",
        "\n",
        "    return alpha  # El mejor conjunto de hiperparámetros\n",
        "\n",
        "# Definición de los hiperparámetros con sus rangos\n",
        "param_ranges = {\n",
        "    \"num_layers\": [2, 3, 4, 5],  # Número de capas convolucionales\n",
        "    \"num_filters\": [16, 32, 64, 128, 256],  # Número de filtros en cada capa\n",
        "    \"batch_norm\": [\"true\", \"false\"],  # Usar o no Batch Normalization\n",
        "    \"epochs\": (20, 50),  # Número de épocas\n",
        "    \"batch_size\": [16, 32, 64],  # Tamaño del lote\n",
        "    \"lr\": (0.0001, 0.01)  # Tasa de aprendizaje\n",
        "}\n",
        "\n",
        "# Parámetros de la ejecución\n",
        "num_lobos = 10      # Tamaño de la población (lobos)\n",
        "maxIter = 50       # Número de iteraciones\n",
        "\n",
        "w1 = 0.85\n",
        "w2 = 0.1\n",
        "w3 = 0.05\n",
        "\n",
        "best_wolf = gwo_optimize(nwolves=10, max_iter=50, w1, w2, w3, param_ranges=param_ranges)\n",
        "\n",
        "# Guardar los hiperparámetros óptimos obtenidos\n",
        "print(\"Mejor conjunto de hiperparámetros encontrados:\", best_wolf)\n",
        "\n",
        "# Ejecucion del algoritmo GWO\n",
        "start_time_cs = time.time()\n",
        "best_nest, best_value, best_accuracy, best_loss, best_time,history, best_nest_per_iteration,best_accuracy_per_iteration, best_loss_per_iteration, best_execution_time_per_iteration, best_value_per_iteration = ejecutarGWO(num_lobos, dimension, maxIter, min_lim, max_lim, w1, w2, w3)\n",
        "end_time_cs = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Inicializar población de lobos (soluciones aleatorias)\n",
        "def initialize_population(nwolves, param_ranges):\n",
        "    population = []\n",
        "    for _ in range(nwolves):\n",
        "        wolf = {}\n",
        "        for param in param_ranges:\n",
        "            if isinstance(param_ranges[param], list):\n",
        "                wolf[param] = random.choice(param_ranges[param])\n",
        "            else:\n",
        "                wolf[param] = random.uniform(*param_ranges[param])\n",
        "        population.append(wolf)\n",
        "    return population\n",
        "\n",
        "# Grey Wolf Optimizer (GWO)\n",
        "def gwo_optimize(nwolves, max_iter, w1, w2, w3, param_ranges):\n",
        "    a = 2  # Coeficiente inicial para GWO\n",
        "\n",
        "    # Inicialización de la población de lobos\n",
        "    wolves = initialize_population(nwolves, param_ranges)\n",
        "\n",
        "    for t in range(max_iter):\n",
        "        a = 2 - t * ((2) / maxIter)  # Disminución de a linealmente de 2 a 0\n",
        "        print(\"\\n**********************************************  ITERACION \", i+1, \" **********************************************\")\n",
        "\n",
        "        # Evaluar el fitness de cada lobo (hiperparámetros)\n",
        "        fitness_scores = []\n",
        "        for wolf in wolves:\n",
        "            accuracy, loss, time_taken = train_cnn_model(\n",
        "                num_conv_layers=wolf['num_layers'],\n",
        "                base_filter_value=wolf['num_filters'],\n",
        "                use_batch_norm=wolf['batch_norm'],\n",
        "                lr=wolf['lr'],\n",
        "                batch_size=wolf['batch_size'],\n",
        "                epochs=wolf['epochs']\n",
        "            )\n",
        "            fitness = w1 * (1 - accuracy / 100) + w2 * loss + w3 * time_taken\n",
        "            fitness_scores.append(fitness)\n",
        "\n",
        "        # Ordenar lobos según fitness (menor es mejor)\n",
        "        sorted_wolves = sorted(zip(fitness_scores, wolves), key=lambda x: x[0])\n",
        "        alpha, beta, delta = sorted_wolves[0][1], sorted_wolves[1][1], sorted_wolves[2][1]\n",
        "\n",
        "        # Actualizar posiciones de los lobos restantes\n",
        "        for i, wolf in enumerate(wolves):\n",
        "            for param in wolf:\n",
        "                if isinstance(wolf[param], (int, float)):\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A1 = 2 * a * r1 - a\n",
        "                    C1 = 2 * r2\n",
        "                    D_alpha = abs(C1 * alpha[param] - wolf[param])\n",
        "                    X1 = alpha[param] - A1 * D_alpha\n",
        "\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A2 = 2 * a * r1 - a\n",
        "                    C2 = 2 * r2\n",
        "                    D_beta = abs(C2 * beta[param] - wolf[param])\n",
        "                    X2 = beta[param] - A2 * D_beta\n",
        "\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A3 = 2 * a * r1 - a\n",
        "                    C3 = 2 * r2\n",
        "                    D_delta = abs(C3 * delta[param] - wolf[param])\n",
        "                    X3 = delta[param] - A3 * D_delta\n",
        "\n",
        "                    # Actualización de la posición\n",
        "                    wolf[param] = (X1 + X2 + X3) / 3\n",
        "\n",
        "\n",
        "\n",
        "    return alpha  # El mejor conjunto de hiperparámetros\n",
        "\n",
        "# Definición de los hiperparámetros con sus rangos\n",
        "param_ranges = {\n",
        "    \"num_layers\": [2, 3, 4, 5],  # Número de capas convolucionales\n",
        "    \"num_filters\": [16, 32, 64, 128, 256],  # Número de filtros en cada capa\n",
        "    \"batch_norm\": [\"true\", \"false\"],  # Usar o no Batch Normalization\n",
        "    \"epochs\": (20, 50),  # Número de épocas\n",
        "    \"batch_size\": [16, 32, 64],  # Tamaño del lote\n",
        "    \"lr\": (0.0001, 0.01)  # Tasa de aprendizaje\n",
        "}\n",
        "\n",
        "# Parámetros de la ejecución\n",
        "num_lobos = 10      # Tamaño de la población (lobos)\n",
        "maxIter = 50       # Número de iteraciones\n",
        "\n",
        "w1 = 0.85\n",
        "w2 = 0.1\n",
        "w3 = 0.05\n",
        "\n",
        "best_wolf = gwo_optimize(nwolves=10, max_iter=50, w1, w2, w3, param_ranges=param_ranges)\n",
        "\n",
        "# Guardar los hiperparámetros óptimos obtenidos\n",
        "print(\"Mejor conjunto de hiperparámetros encontrados:\", best_wolf)\n"
      ],
      "metadata": {
        "id": "X9XoZqE_iEI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}