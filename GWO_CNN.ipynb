{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyhkb4UVPaHicm4ryf9QvG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukipuki0/GWO/blob/main/GWO_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIXMql-BhbSQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.special import gamma\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Librerias para CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "#link github: https://github.com/lukipuki0/GWO\n",
        "# Configuración de visualización de Pandas  (No para HPC)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#pd.set_option('display.width', 200)\n",
        "#pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#************************************************************************************************************\n",
        "# *************************FUNCION PARA CNN *****************************************************************\n",
        "def train_cnn_model(num_conv_layers, base_filter_value, use_batch_norm, lr, batch_size, epochs):\n",
        "    start_time = time.time()  # inicio tiempo\n",
        "    now = datetime.datetime.now()\n",
        "    formatted_date = now.strftime('%Y%m%d_%H%M%S')\n",
        "    #Directorio donde se guardarán las ejecuiones de CNN --cambiar nombre segun configuracion de esquema a aprobar\n",
        "\n",
        "    base_directory = 'Esquema1'  # Ruta directa en HPC\n",
        "    #base_directory='/content/drive/MyDrive/Colab Notebooks/Esquema1'\n",
        "\n",
        "    folder_name = os.path.join(base_directory, f'ejecucion_cnn_{formatted_date}')\n",
        "\n",
        "    # Revisar existencia de directorio\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "    log_file_path = os.path.join(folder_name, 'training_log.txt')\n",
        "\n",
        "    class CustomCNN(nn.Module):\n",
        "        def __init__(self, num_conv_layers, base_filter_value, use_batch_norm):\n",
        "            super(CustomCNN, self).__init__()\n",
        "            layers = []\n",
        "            in_channels = 3  # Assuming input images have 3 channels (RGB)\n",
        "            current_filters = base_filter_value\n",
        "\n",
        "            for i in range(num_conv_layers):\n",
        "                layers.append(nn.Conv2d(in_channels, current_filters, kernel_size=3, padding=1))\n",
        "                if use_batch_norm:\n",
        "                    layers.append(nn.BatchNorm2d(current_filters))\n",
        "                layers.append(nn.ReLU(inplace=True))\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "                in_channels = current_filters\n",
        "                current_filters *= 2  # Double the number of filters for the next layer\n",
        "\n",
        "            self.features = nn.Sequential(*layers)\n",
        "            self.fc1 = nn.Linear(in_channels * (64 // 2**num_conv_layers)**2, 1024)\n",
        "            self.dropout = nn.Dropout(0.5)\n",
        "            self.fc2 = nn.Linear(1024, 2)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.features(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.dropout(x)\n",
        "            x = self.fc2(x)\n",
        "            return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Data loading and transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    #Ruta de acceso local en colab\n",
        "    #train_path = '/content/drive/MyDrive/Colab Notebooks/dataset/train'\n",
        "    #valid_path ='/content/drive/MyDrive/Colab Notebooks/dataset/valid'\n",
        "    #test_path = '/content/drive/MyDrive/Colab Notebooks/dataset/test'\n",
        "\n",
        "    #Ruta de acceso en HPC\n",
        "    train_path = '../../Datasets/dataset_experimental/train'\n",
        "    valid_path = '../../Datasets/dataset_experimental/valid'\n",
        "    test_path = '../../Datasets/dataset_experimental/test'\n",
        "\n",
        "    train_data = datasets.ImageFolder(train_path, transform=transform)\n",
        "    valid_data = datasets.ImageFolder(valid_path, transform=transform)\n",
        "    test_data = datasets.ImageFolder(test_path, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_data, batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
        "\n",
        "    model = CustomCNN(num_conv_layers, base_filter_value, use_batch_norm)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loss_history, valid_loss_history, valid_accuracy_history = [], [], []\n",
        "\n",
        "    with open(log_file_path, 'w') as log_file:\n",
        "        print('\\nResultados por epochs')\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "            for data, target in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            average_train_loss = total_train_loss / len(train_loader.dataset)\n",
        "            train_loss_history.append(average_train_loss)\n",
        "\n",
        "            model.eval()\n",
        "            total_valid_loss, valid_correct, total_valid_samples = 0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in valid_loader:\n",
        "                    output = model(data)\n",
        "                    loss = criterion(output, target)\n",
        "                    total_valid_loss += loss.item() * data.size(0)\n",
        "                    pred = output.argmax(dim=1, keepdim=True)\n",
        "                    valid_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "                    total_valid_samples += data.size(0)\n",
        "\n",
        "            average_valid_loss = total_valid_loss / total_valid_samples\n",
        "            valid_accuracy = 100. * valid_correct / total_valid_samples\n",
        "            valid_loss_history.append(average_valid_loss)\n",
        "            valid_accuracy_history.append(valid_accuracy)\n",
        "            log_msg = f'Epoch {epoch+1}/{epochs}, Training Loss: {average_train_loss:.4f}, Validation Loss: {average_valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.2f}%\\n'\n",
        "            log_file.write(log_msg)\n",
        "            print(log_msg)\n",
        "\n",
        "        # Calculate the final average values\n",
        "        final_average_valid_loss = sum(valid_loss_history) / len(valid_loss_history)\n",
        "        final_average_valid_accuracy = sum(valid_accuracy_history) / len(valid_accuracy_history)\n",
        "\n",
        "        # Calculate total execution time\n",
        "        end_time = time.time()\n",
        "        total_execution_time = end_time - start_time\n",
        "\n",
        "        # Convert execution time to hours, minutes, and seconds\n",
        "        hours, rem = divmod(total_execution_time, 3600)\n",
        "        minutes, seconds = divmod(rem, 60)\n",
        "        time_formatted = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n",
        "\n",
        "        # Log final results\n",
        "        final_log_msg = f'\\nFinal Average Validation Loss: {final_average_valid_loss:.4f}\\n'\n",
        "        final_log_msg += f'Final Average Validation Accuracy: {final_average_valid_accuracy:.2f}%\\n'\n",
        "        final_log_msg += f'Total Execution Time: {time_formatted} (hh:mm:ss)\\n'\n",
        "        log_file.write(final_log_msg)\n",
        "        print(final_log_msg)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_loss_history, label='Training Loss')\n",
        "    plt.plot(valid_loss_history, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(folder_name, 'loss_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot validation accuracy\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(valid_accuracy_history, label='Validation Accuracy')\n",
        "    plt.title('Validation Accuracy per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(folder_name, 'accuracy_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return final_average_valid_loss, final_average_valid_accuracy, total_execution_time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ejecucion del algoritmo GWO\n",
        "start_time_cs = time.time()\n",
        "best_nest, best_value, best_accuracy, best_loss, best_time,history, best_nest_per_iteration,best_accuracy_per_iteration, best_loss_per_iteration, best_execution_time_per_iteration, best_value_per_iteration = ejecutarGWO(num_lobos, dimension, maxIter, min_lim, max_lim, w1, w2, w3)\n",
        "end_time_cs = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "#************************************************************************************************************\n",
        "#**********************************  G R E Y   W O L F   O P T I M A Z E R  *********************************\n",
        "\n",
        "# Grey Wolf Optimizer (GWO)\n",
        "def gwo_optimize(wolves, max_iter, w1, w2, w3):\n",
        "\n",
        "    #guardar el mejor valor durante la ejecucion\n",
        "    history = []\n",
        "    best_wolf = None\n",
        "    best_value = float('inf')\n",
        "    best_accuracy = 0\n",
        "    best_loss = 0\n",
        "    best_time = 0\n",
        "\n",
        "    #Historial de datos por iteracion\n",
        "    best_accuracy_per_iteration = []\n",
        "    best_loss_per_iteration = []\n",
        "    best_execution_time_per_iteration = []\n",
        "    best_value_per_iteration = []\n",
        "    best_wolf_per_iteration=[]\n",
        "\n",
        "    #Ruta para HPC\n",
        "    history_file_path = os.path.join('Esquema1/Historial', 'history.txt')\n",
        "\n",
        "\n",
        "    with open(history_file_path, 'w') as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    #calcular antes de los movimientos y actualizar lobos\n",
        "    for wolf in wolves:\n",
        "        final_average_valid_loss, final_average_valid_accuracy, total_execution_time = train_cnn_model(\n",
        "            num_conv_layers=wolf['num_layers'],\n",
        "            base_filter_value=wolf['num_filters'],\n",
        "            use_batch_norm=wolf['batch_norm'],\n",
        "            lr=wolf['lr'],\n",
        "            batch_size=wolf['batch_size'],\n",
        "            epochs=wolf['epochs']\n",
        "        )\n",
        "        current_value = w1 * (1 - final_average_valid_accuracy / 100 + w2 * final_average_valid_loss + w3 * total_execution_time)\n",
        "        fitness_scores.append(current_value)\n",
        "\n",
        "        print(\"\\nLobo actual\", wolf)\n",
        "        print(\"\\nAverage accuracy\", final_average_valid_accuracy)\n",
        "        print(\"\\nAverage Loss\", final_average_valid_loss)\n",
        "        print(\"\\nValor funcion objetivo\", current_value)\n",
        "        print(\"\\nTiempo total en segundos \", total_execution_time)\n",
        "\n",
        "        # Guardar historial\n",
        "        if current_value < best_value:\n",
        "            best_value = current_value\n",
        "            best_accuracy = final_average_valid_accuracy\n",
        "            best_loss = final_average_valid_loss\n",
        "            best_time = total_execution_time\n",
        "            best_wolf = wolf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for t in range(max_iter):\n",
        "\n",
        "        print(\"\\n**********************************************  ITERACION \", i+1, \" **********************************************\")\n",
        "        a = 2 - t * ((2) / maxIter)  # Disminución de a linealmente de 2 a 0\n",
        "\n",
        "        fitness_scores = []\n",
        "\n",
        "        # Ordenar lobos según fitness\n",
        "        sorted_wolves = sorted(zip(fitness_scores, wolves), key=lambda x: x[0])\n",
        "        alpha, beta, delta = sorted_wolves[0][1], sorted_wolves[1][1], sorted_wolves[2][1]\n",
        "\n",
        "        # Actualizar posiciones de los lobos restantes\n",
        "        for i, wolf in enumerate(wolves):\n",
        "            for param in wolf:\n",
        "\n",
        "                # Ignorar la actualización de Batch Norm (parámetro discreto)\n",
        "                if param == 'batch_norm':\n",
        "                    continue  # No actualizar el parámetro batch_norm (discreto)\n",
        "\n",
        "                if isinstance(wolf[param], (int, float)):\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A1 = 2 * a * r1 - a\n",
        "                    C1 = 2 * r2\n",
        "                    D_alpha = abs(C1 * alpha[param] - wolf[param])\n",
        "                    X1 = alpha[param] - A1 * D_alpha\n",
        "\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A2 = 2 * a * r1 - a\n",
        "                    C2 = 2 * r2\n",
        "                    D_beta = abs(C2 * beta[param] - wolf[param])\n",
        "                    X2 = beta[param] - A2 * D_beta\n",
        "\n",
        "                    r1, r2 = random.random(), random.random()\n",
        "                    A3 = 2 * a * r1 - a\n",
        "                    C3 = 2 * r2\n",
        "                    D_delta = abs(C3 * delta[param] - wolf[param])\n",
        "                    X3 = delta[param] - A3 * D_delta\n",
        "\n",
        "                    # Actualización de la posición\n",
        "                    wolf[param] = (X1 + X2 + X3) / 3\n",
        "\n",
        "\n",
        "    return alpha  # El mejor conjunto de hiperparámetros\n",
        "\n",
        "# Inicializar población de lobos (soluciones aleatorias)\n",
        "def initialize_population(param_ranges):\n",
        "    wolf = {}\n",
        "    for param, options in param_ranges.items():\n",
        "        if isinstance(options, list):  # Si las opciones son una lista\n",
        "            wolf[param] = random.choice(options)  # Selecciona un valor aleatorio de la lista\n",
        "        elif isinstance(options, tuple):  # Para rangos num�ricos, como 'lr' y epochs\n",
        "            wolf[param] = round(random.uniform(options[0], options[1]), 4)  # Genera un valor flotante aleatorio para lr, redondeado en 4 decimales\n",
        "            if param == \"epochs\":\n",
        "                wolf[param] = int(wolf[param])  # Asegurar que epochs sea un entero\n",
        "    return wolf\n",
        "\n",
        "\n",
        "\n",
        "# Definición de los hiperparámetros con sus rangos\n",
        "param_ranges = {\n",
        "    \"num_layers\": [2, 3, 4, 5],  # Número de capas convolucionales\n",
        "    \"num_filters\": [16, 32, 64, 128, 256],  # Número de filtros en cada capa\n",
        "    \"batch_norm\": [\"true\", \"false\"],  # Usar o no Batch Normalization\n",
        "    \"epochs\": (20, 50),  # Número de épocas\n",
        "    \"batch_size\": [16, 32, 64],  # Tamaño del lote\n",
        "    \"lr\": (0.0001, 0.01)  # Tasa de aprendizaje\n",
        "}\n",
        "\n",
        "# Parámetros de la ejecución\n",
        "nwolves = 10      # Tamaño de la población (lobos)\n",
        "maxIter = 10       # Número de iteraciones\n",
        "\n",
        "w1 = 0.85\n",
        "w2 = 0.1\n",
        "w3 = 0.05\n",
        "\n",
        "# Inicialización de la población de lobos\n",
        "wolves = [initialize_population(param_ranges) for _ in range(nwolves)]\n",
        "\n",
        "# Ejecucion del algoritmo GWO\n",
        "start_time_cs = time.time()\n",
        "best_wolf, best_value, best_accuracy, best_loss, best_time,history, best_wolf_per_iteration,best_accuracy_per_iteration, best_loss_per_iteration, best_execution_time_per_iteration, best_value_per_iteration = ejecutarGWO(wolves,maxIter, w1, w2, w3)\n",
        "end_time_cs = time.time()\n",
        "\n",
        "best_wolf = gwo_optimize(wolves,maxIter, w1, w2, w3)\n",
        "\n",
        "# Guardar los hiperparámetros óptimos obtenidos\n",
        "print(\"Mejor conjunto de hiperparámetros encontrados:\", best_wolf)\n"
      ],
      "metadata": {
        "id": "X9XoZqE_iEI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Inicializar población de lobos (soluciones aleatorias)\n",
        "\n",
        "\n",
        "# Inicializar población de lobos (soluciones aleatorias)\n",
        "\n",
        "\n",
        "def initialize_population(param_ranges):\n",
        "    wolf = {}\n",
        "    for param, options in param_ranges.items():\n",
        "        if isinstance(options, list):  # Si las opciones son una lista\n",
        "            wolf[param] = random.choice(options)  # Selecciona un valor aleatorio de la lista\n",
        "        elif isinstance(options, tuple):  # Para rangos num�ricos, como 'lr' y epochs\n",
        "            wolf[param] = round(random.uniform(options[0], options[1]), 4)  # Genera un valor flotante aleatorio para lr, redondeado en 4 decimales\n",
        "            if param == \"epochs\":\n",
        "                wolf[param] = int(wolf[param])  # Asegurar que epochs sea un entero\n",
        "    return wolf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Definición de los hiperparámetros con sus rangos\n",
        "param_ranges = {\n",
        "    \"num_layers\": [2, 3, 4, 5],  # Número de capas convolucionales\n",
        "    \"num_filters\": [16, 32, 64, 128, 256],  # Número de filtros en cada capa\n",
        "    \"batch_norm\": [\"true\", \"false\"],  # Usar o no Batch Normalization\n",
        "    \"epochs\": (20, 50),  # Número de épocas\n",
        "    \"batch_size\": [16, 32, 64],  # Tamaño del lote\n",
        "    \"lr\": (0.0001, 0.01)  # Tasa de aprendizaje\n",
        "}\n",
        "nwolves = 10\n",
        "# Inicialización de la población de lobos\n",
        "wolves = [initialize_population(param_ranges) for _ in range(nwolves)]\n",
        "#wolves = initialize_population(nwolves, param_ranges)\n",
        "\n",
        "wolves\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNi7WtNiCOab",
        "outputId": "26ea53e2-bf61-4df2-dc81-2d102d896028"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'num_layers': 5,\n",
              "  'num_filters': 16,\n",
              "  'batch_norm': 'false',\n",
              "  'epochs': 49,\n",
              "  'batch_size': 32,\n",
              "  'lr': 0.0081},\n",
              " {'num_layers': 4,\n",
              "  'num_filters': 256,\n",
              "  'batch_norm': 'false',\n",
              "  'epochs': 22,\n",
              "  'batch_size': 64,\n",
              "  'lr': 0.0092},\n",
              " {'num_layers': 3,\n",
              "  'num_filters': 128,\n",
              "  'batch_norm': 'true',\n",
              "  'epochs': 35,\n",
              "  'batch_size': 64,\n",
              "  'lr': 0.0001},\n",
              " {'num_layers': 2,\n",
              "  'num_filters': 16,\n",
              "  'batch_norm': 'true',\n",
              "  'epochs': 23,\n",
              "  'batch_size': 32,\n",
              "  'lr': 0.0033},\n",
              " {'num_layers': 5,\n",
              "  'num_filters': 64,\n",
              "  'batch_norm': 'false',\n",
              "  'epochs': 26,\n",
              "  'batch_size': 16,\n",
              "  'lr': 0.0075},\n",
              " {'num_layers': 5,\n",
              "  'num_filters': 256,\n",
              "  'batch_norm': 'false',\n",
              "  'epochs': 39,\n",
              "  'batch_size': 64,\n",
              "  'lr': 0.0017},\n",
              " {'num_layers': 5,\n",
              "  'num_filters': 64,\n",
              "  'batch_norm': 'true',\n",
              "  'epochs': 22,\n",
              "  'batch_size': 16,\n",
              "  'lr': 0.0004},\n",
              " {'num_layers': 3,\n",
              "  'num_filters': 128,\n",
              "  'batch_norm': 'false',\n",
              "  'epochs': 23,\n",
              "  'batch_size': 64,\n",
              "  'lr': 0.0062},\n",
              " {'num_layers': 5,\n",
              "  'num_filters': 256,\n",
              "  'batch_norm': 'true',\n",
              "  'epochs': 22,\n",
              "  'batch_size': 64,\n",
              "  'lr': 0.0065},\n",
              " {'num_layers': 5,\n",
              "  'num_filters': 256,\n",
              "  'batch_norm': 'false',\n",
              "  'epochs': 36,\n",
              "  'batch_size': 32,\n",
              "  'lr': 0.0008}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}